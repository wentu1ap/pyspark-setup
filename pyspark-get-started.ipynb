{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ecff06-72b4-44ce-b9b8-aa481204f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PySpark environment variables\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/Users/drewwenturine/Coding/Apps/Spark\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3606931-ee6a-44bf-81e3-fa6ccbad2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "#import lit library\n",
    "from pyspark.sql.functions import lit\n",
    "#import concat library\n",
    "from pyspark.sql.functions import concat\n",
    "#import coalesce library\n",
    "from pyspark.sql.functions import coalesce\n",
    "#import functions library\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996a95a2-8a1a-4052-a2a9-d25ba6f9c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/15 10:35:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark-get-started\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5d627eb-fab8-4dca-a620-5c185ee746b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|   Drew| 12|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "|  Woody| 42|\n",
      "+-------+---+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Age_Bucket: string (nullable = true)\n",
      "\n",
      "+---+----------+\n",
      "|Age|Age_Bucket|\n",
      "+---+----------+\n",
      "| 12|     10-20|\n",
      "| 30|     30-40|\n",
      "| 35|     30-40|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create df\n",
    "data = [(\"Drew\", 12),(\"Bob\", 30),(\"Charlie\", 35),(\"Woody\", 42)]\n",
    "age_buckets = [(12, \"10-20\"),(30, \"30-40\"),(35, \"30-40\")]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df_buckets = spark.createDataFrame(age_buckets, [\"Age\", \"Age_Bucket\"])\n",
    "\n",
    "print(type(df))\n",
    "df.printSchema()\n",
    "df.show()\n",
    "print(type(df_buckets))\n",
    "df_buckets.printSchema()\n",
    "df_buckets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab086e0-705b-4d06-a339-0ee608c2c730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   Name|\n",
      "+-------+\n",
      "|   Drew|\n",
      "|    Bob|\n",
      "|Charlie|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select\n",
    "# Glue Method\n",
    "# df_sel = df.select_fields ([\"Name\"])\n",
    "\n",
    "# PySpark Method\n",
    "df_sel = df.select(\"Name\")\n",
    "\n",
    "\n",
    "df_sel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeaf3a69-0b89-47a2-bc0c-7157224f1cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|Age|\n",
      "+---+\n",
      "| 12|\n",
      "| 30|\n",
      "| 35|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop\n",
    "df_drop = df.select(\"Name\", \"Age\")\n",
    "\n",
    "#Glue Method\n",
    "# df_drop.drop_fields([\"Name\"])\n",
    "\n",
    "# PySpark Method\n",
    "df_drop = df_drop.drop(\"Name\")\n",
    "\n",
    "df_drop.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2d59d4c-d176-43a8-8d84-674af4b83e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|  Names|Age|\n",
      "+-------+---+\n",
      "|   Drew| 12|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename Columns\n",
    "df_rename = df.select(\"Name\", \"Age\")\n",
    "\n",
    "#Glue Method\n",
    "# mapping = [(\"Name\", \"string\", \"Names\", \"string\"), (\"Age\", \"long\", \"Age\", \"long\")]\n",
    "\n",
    "# df_map = ApplyMapping.apply(\n",
    "# frame = df_rename,\n",
    "# mappings = mapping,\n",
    "# trasformation_ctx = 'applymapping1'    \n",
    "# )\n",
    "\n",
    "# df_map.show()\n",
    "\n",
    "\n",
    "# PySpark Method\n",
    "df_rename = df_rename.withColumnRenamed(\"Name\", \"Names\")\n",
    "\n",
    "df_rename.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "792791d7-42a5-4a91-9cca-e1e8d77318fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|Drew| 12|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter\n",
    "df_filter = df.select(\"Name\", \"Age\")\n",
    "\n",
    "#Glue Method\n",
    "#df_filter = Filter.apply(\n",
    "#frame = df_filter,\n",
    "#f = lambda x: x[\"Name\" in \"Drew\"]\n",
    "#)\n",
    "\n",
    "# PySpark Method\n",
    "#df_filter = df_filter.filter(df_filter.Name == \"Drew\")\n",
    "df_filter = df_filter.where(\"Name == 'Drew'\")\n",
    "\n",
    "\n",
    "df_filter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9061875b-1b8b-4e31-a666-23204eb09c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------------+\n",
      "|   Name|Age|      Date|    Name-Age|\n",
      "+-------+---+----------+------------+\n",
      "|   Drew| 12|2023-08-15|   Drew - 12|\n",
      "|    Bob| 30|2023-08-15|    Bob - 30|\n",
      "|Charlie| 35|2023-08-15|Charlie - 35|\n",
      "+-------+---+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add Column\n",
    "# NEED TO HAVE lit library imported\n",
    "\n",
    "#Glue Method\n",
    "#not possible with glue method\n",
    "\n",
    "# PySpark Method\n",
    "df_new_column = df.select(\"Name\", \"Age\")\n",
    "\n",
    "df_new_column = df_new_column.withColumn(\"Date\", lit(\"2023-08-15\"))\n",
    "df_new_column = df_new_column.withColumn(\"Name-Age\", concat(\"Name\", lit(\" - \"), \"Age\"))\n",
    "\n",
    "df_new_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67ce3137-ee50-4a77-819b-b0c3d72eaf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   Name|sum(count)|\n",
      "+-------+----------+\n",
      "|   Drew|         1|\n",
      "|    Bob|         1|\n",
      "|Charlie|         1|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping/Aggregate\n",
    "df_group = df.select(\"Name\", \"Age\")\n",
    "\n",
    "# PySpark Method\n",
    "df_group = df_group.groupBy(\"Name\").count()\n",
    "#df_group = df_group.groupBy(\"Name\").sum()\n",
    "\n",
    "df_group.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24b97708-7cf6-4310-87b6-c538a5731444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+----------+\n",
      "|   Name|Age|Age|Age_Bucket|\n",
      "+-------+---+---+----------+\n",
      "|   Drew| 12| 12|     10-20|\n",
      "|    Bob| 30| 30|     30-40|\n",
      "|Charlie| 35| 35|     30-40|\n",
      "+-------+---+---+----------+\n",
      "\n",
      "Before default values for NULLs\n",
      "+-------+----+----------+\n",
      "|   Name| Age|Age_Bucket|\n",
      "+-------+----+----------+\n",
      "|   Drew|  12|     10-20|\n",
      "|    Bob|  30|     30-40|\n",
      "|Charlie|  35|     30-40|\n",
      "|  Woody|null|      null|\n",
      "+-------+----+----------+\n",
      "\n",
      "After default values for NULLs\n",
      "+-------+---+----------+----------------+\n",
      "|   Name|Age|Age_Bucket|   Age_defaulted|\n",
      "+-------+---+----------+----------------+\n",
      "|   Drew| 12|     10-20|              12|\n",
      "|    Bob| 30|     30-40|              30|\n",
      "|Charlie| 35|     30-40|              35|\n",
      "|  Woody| -1|   unknown|someDefaultValue|\n",
      "+-------+---+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join\n",
    "df_join = df.select(\"Name\", \"Age\")\n",
    "\n",
    "#Glue Method\n",
    "#df_join = df.join([\"Age\"],[\"Age\"],df_buckets)\n",
    "\n",
    "#df_join.show()\n",
    "\n",
    "\n",
    "# PySpark Method\n",
    "#reading from S3\n",
    "#new_df = glueContext.create_dynamic_frame.from_catalog(\n",
    "#    database = \"db\",\n",
    "#    table_name = \"table\"\n",
    "#).toDF()\n",
    "\n",
    "#inner\n",
    "df_inner_join = df_join.join(df_buckets,df_join.Age == df_buckets.Age,\"inner\")\n",
    "\n",
    "df_inner_join.show()\n",
    "\n",
    "#left\n",
    "df_left_join = df_join.join(df_buckets,df_join.Age == df_buckets.Age,\"left\")\n",
    "df_left_join = df_left_join.drop(df_join.Age)\n",
    "\n",
    "print(\"Before default values for NULLs\")\n",
    "df_left_join.show()\n",
    "\n",
    "\n",
    "# 2 ways to fill nulls\n",
    "df_left_join = df_left_join.withColumn(\"Age_defaulted\", coalesce(F.col(\"Age\"), lit(\"someDefaultValue\")))\n",
    "df_left_join = df_left_join.fillna( {'Age': -1, 'Age_Bucket': 'unknown'} )\n",
    "\n",
    "print(\"After default values for NULLs\")\n",
    "df_left_join.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578217e0-7ed8-4381-884f-519500f546bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write spark df to S3\n",
    "\n",
    "# Import Dynamic DataFrame class\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "#Convert from Spark Data Frame to Glue Dynamic Frame\n",
    "dyfCustomersConvert = DynamicFrame.fromDF(sparkDf, glueContext, \"convert\")\n",
    "\n",
    "#Show converted Glue Dynamic Frame\n",
    "dyfCustomersConvert.show()\n",
    "\n",
    "\n",
    "# write down the data in converted Dynamic Frame to S3 location. \n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "                            frame = dyfCustomersConvert,\n",
    "                            connection_type=\"s3\", \n",
    "                            connection_options = {\"path\": \"s3://<YOUR_S3_BUCKET_NAME>/write_down_dyf_to_s3\"}, \n",
    "                            format = \"csv\", \n",
    "                            format_options={\n",
    "                                \"separator\": \",\"\n",
    "                                },\n",
    "                            transformation_ctx = \"datasink2\")\n",
    "\n",
    "# write data from the converted to customers_write_dyf table using the meta data stored in the glue data catalog \n",
    "glueContext.write_dynamic_frame.from_catalog(\n",
    "    frame = dyfCustomersConvert,\n",
    "    database = \"pyspark_tutorial_db\",  \n",
    "    table_name = \"customers_write_dyf\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
